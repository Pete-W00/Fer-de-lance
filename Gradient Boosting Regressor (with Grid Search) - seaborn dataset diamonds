# Gradient Boosting Regressor - seaborn dataset diamonds 
# Predicting diamond price from carat

'''
Summary:
Model one = Linear regression --> 84.9% model accuracy (1d.p.)
Model two = GradientBoostingRegressor(random_state = 42) --> increased accuracy of model one by about 3%
Model three = GradientBoostingRegressor(learning_rate = 0.01, n_estimators = 450, random_state = 42) --> increased accuracy of model two by about 0.02%
(Model three was obtained using grid search on learning rates and number of iterations)
'''


import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

import matplotlib
from matplotlib import pyplot as plt

import pandas
from pandas import DataFrame

import seaborn as sns

df = sns.load_dataset("diamonds")
df

df2 = df[["carat", "price"]]                                                                               # The only two variables with a reasonable correlation
df2

df2.isnull().sum()                                                                                         # Clean


plt.figure(figsize = (30, 15))                                                                             # Expand plot horizontally

plt.scatter(df2.carat, df2.price, label = "carat", marker = 'x', color = 'g')

plt.xlabel("diamond carat")
plt.ylabel("price")
plt.title("Diamond prices based on carat")
plt.legend()

plt.show()                                                                                                 # Some positive correlation


X = df2.carat  
y = df2.price

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)

X_train = X_train.values.reshape(-1, 1)                                                                    # Reshape X_train data to 2D, but leave y_train (otherwise ValueError)
X_test = X_test.values.reshape(-1, 1)                                                                      # Reshape X_test data to 2D, but leave y_test (otherwise ValueError)


# See how a linear regression model does and then use gradient boosting regressor for improvement

LR = LinearRegression()         
LR.fit(X_train, y_train)
LR.score(X_test, y_test)                                                                                   # 0.8489390686155808


GBR = GradientBoostingRegressor(random_state = 42)

GBR.fit(X_train, y_train)                                                                                  # Use default parameters for now

GBR.score(X_test, y_test)                                                                                  # 0.8727609484785352 (improvement by about 3%)


# Model tunings - Gridsearch CV with learning rate (lr*model_residuals) and number of iterations

parameters = {"learning_rate" : [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 
              "n_estimators": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]}

model = GridSearchCV(GBR, parameters, cv = 3)

model.fit(X_test, y_test)                                                                                  # This takes some time (about 10mins)

cvresults = DataFrame(model.cv_results_)
cvresults

cvresults_compact = cvresults[["param_learning_rate", 
                               "param_n_estimators", 
                               "mean_test_score",
                               "std_test_score",
                               "rank_test_score"]]
                               
cvresults_compact

for i in cvresults_compact.rank_test_score:
    print(i)                                                                                               # There is only one with rank one (good)
    
cvresults_compact.head(20)                                                                                 # Optimality is learning rate = 0.01 and n_estimators = 450

GBR2 = GradientBoostingRegressor(learning_rate = 0.01, n_estimators = 450, random_state = 42)

GBR2.fit(X_train, y_train)

GBR2.score(X_test, y_test)                                                                                 # 0.8729725675398942

improvement = 0.8729725675398942 - 0.8727609484785352                                                      # This model acc. - previous model acc
print(improvement)                                                                                         # Model improved by 0.00021161906135891417 (0.02%) - very small, but an improvement using grid search


# Use model for predictions

pred1 = GBR2.predict([[0.1]])
pred2 = GBR2.predict([[0.8]])
pred3 = GBR2.predict([[0.29]])

print(pred1)                                                                                               # 611 (nearest integer)
print(pred2)                                                                                               # 3128 (nearest integer)
print(pred3)                                                                                               # 672 (nearest integer)
