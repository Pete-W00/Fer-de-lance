# UCI Adult Census Income (dataset from Kaggle)

# I do not own this dataset - I am simply doing some analysis on it, no copyright intended. Citation: 
# Author - UCI Machine Learning
# Original source - https://www.kaggle.com/uciml/adult-census-income?select=adult.csv


'''
Summary:
After cleaning the dataframe, I did some basic visualisations.
I then sampled 7000 of each income type (otherwise the machine learning models are biased)
I then split the sampled dataframe into (A) categorical data and (B) other numeric
(A) - A basic decision tree gave a model accuracy of 75.1%
    - Applying a bagging classifier to the decision tree increased model accuracy to 75.6%
    - A tuned GradientBoostingClassifier increased the accuracy to 78.3%  
    - Conclusion: predicting income with the categorical data - maximum accuracy achieved was 78.3% 
(B) - Using logistic regression was shown to be very inaccurate - in fact the model was worthless as it never predicted an income >50k
    - Logistic regression model - accuracy at 50.5% (e.g. wrong for all the incomes >50k)
Conclusion - use the categorical data and apply classification models to this to obtain best income predictor. Do not use regression.
'''


import pandas
from pandas import read_csv, concat, DataFrame, get_dummies

import seaborn
from seaborn import heatmap

from matplotlib import pyplot as plt

import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression



df = read_csv("ALL.csv")                                                                                                # NB type .csv
df

# Cleaning - remove the '?' entries

df.isnull().sum()                                                                                         # This doesn't pick up the null values because the entry is '?'

df2 = df[df != "?"]                                                                                       # This converts the '?' to NaN - useful trick
df2

df2.isnull().sum()                                                                                        # null values are 'workclass', 'occupation' and 'native.country'
                                                                                                          # Can either drop these columns or drop null values
                                                                                                          # I would prefer to keep the columns
        
heatmap(df2.isnull())                                                                                     # I need to drop three coulmns as well - I typed something in a cell away from the data and this has generated three extra columns

df2 = df2.drop({"Unnamed: 15",                                                                            # This cleaning stage is only necessary if you try to import with data outside the main table
                "Unnamed: 16",                                                                            # Get rid of these extra columns before using drop na otherwise you will delete the entire dataset!
                "https://www.kaggle.com/uciml/adult-census-income?select=adult.csv"}, axis = 1)

df2

df2.dropna(inplace = True)
df2 = df2.reset_index(drop = True)
df2                                                                                                       # Loss of 2399 rows

heatmap(df2.isnull())                                                                                     # Data clean


# Now some general analysis/visualisations (this is in addition to some analysis I have done with SQL)

plt.scatter(df2["age"], df2["education.num"], marker = 'x', color = 'b')                                  # No relationship between age and education.num
plt.xlabel("Age")
plt.ylabel("Education Level")
plt.title("Relationship between Age and Education Level")
plt.show()

salary = get_dummies(data = df2.income, drop_first = True)
salary

df2A = concat([df2, salary], axis = 1)                                                                     # INCLUDE AXIS = 1 OTHERWISE IT CONCATS STRANGELY
df2A

df2A.drop("income", axis = 1, inplace = True)
df2A = df2A.rename(columns = {'>50K': 'income'})
df2A 

plt.scatter(df2A["age"], df2A["income"], marker = 'x', color = 'b')                                        # No relationship between age and income
plt.xlabel("Age")
plt.ylabel("Income")
plt.title("Relationship between Age and Income")
plt.show()

plt.scatter(df2A["education.num"], df2A["income"], marker = 'x', color = 'b')                              # No strong relationship between education.num and income
plt.xlabel("Education.num")
plt.ylabel("Income")
plt.title("Relationship between Education.num and Income")
plt.show()

plt.scatter(df2A["hours.per.week"], df2A["income"], marker = 'x', color = 'b')                             # No relationship between hours.per.week and income
plt.xlabel("hours.per.week")
plt.ylabel("Income")
plt.title("Relationship between hours.per.week and Income")
plt.show()

# No obvious correlations with income in the data
# See how the income is distributed

df2["income"].value_counts()
incomes = df2["income"].value_counts().index.tolist() 
income_counts = df2["income"].value_counts().tolist()

plt.bar(incomes, income_counts, color = "g")
plt.xticks(rotation = 0)
plt.xlabel("Incomes")
plt.ylabel("Counts of incomes")
plt.title("Income distribution")
plt.show()        

genderIncome = df2[["sex", "income"]]
genderIncome

female = genderIncome[genderIncome["sex"] == "Female"]
male = genderIncome[genderIncome["sex"] == "Male"]

female = female.reset_index(drop = True)                                 # There are far fewer females in dataset
male = male.reset_index(drop = True)

male.income.value_counts()

maleIncomes = male.income.value_counts().index.tolist()
maleIncomeCounts = male.income.value_counts().tolist()

plt.bar(maleIncomes, maleIncomeCounts, color = "g")
plt.xticks(rotation = 0)
plt.xlabel("Male Incomes")
plt.ylabel("Counts of male incomes")
plt.title("Income distribution")
plt.show()        

femaleIncomes = female.income.value_counts().index.tolist()
femaleIncomeCounts = female.income.value_counts().tolist()

plt.bar(femaleIncomes, femaleIncomeCounts, color = "g")
plt.xticks(rotation = 0)
plt.xlabel("Female Incomes")
plt.ylabel("Counts of female incomes")
plt.title("Income distribution")
plt.show()        

plt.bar(maleIncomes, maleIncomeCounts, width = 0.6, label = "Male", color = "g")
plt.bar(femaleIncomes, femaleIncomeCounts, width = 0.6, label = "Female", color = "b")

plt.xlabel("Incomes")
plt.ylabel("Counts of incomes")
plt.legend()
plt.title("Income distribution")
plt.show()        



raceGenderIncome = df2[["race", "sex", "income"]]
raceGenderIncome

White = raceGenderIncome[raceGenderIncome["race"] == "White"]
Black = raceGenderIncome[raceGenderIncome["race"] == "Black"]
API = raceGenderIncome[raceGenderIncome["race"] == "Asian-Pac-Islander"]                    # Asian-Pac-Islander 
AIE = raceGenderIncome[raceGenderIncome["race"] == "Amer-Indian-Eskimo"]                    # Amer-Indian-Eskimo 
Other = raceGenderIncome[raceGenderIncome["race"] == "other"]

whiteIncomes = White.income.value_counts().index.tolist()
whiteIncomeCounts = White.income.value_counts().tolist()

blackIncomes = Black.income.value_counts().index.tolist()
blackIncomeCounts = Black.income.value_counts().tolist()

apiIncomes = API.income.value_counts().index.tolist()
apiIncomeCounts = API.income.value_counts().tolist()

aieIncomes = AIE.income.value_counts().index.tolist()
aieIncomeCounts = AIE.income.value_counts().tolist()

otherIncomes = Other.income.value_counts().index.tolist()
otherIncomeCounts = Other.income.value_counts().tolist()

plt.figure(figsize = (10, 10))

plt.bar(whiteIncomes, whiteIncomeCounts, width = 0.9, color = "g", label = "White")
plt.bar(blackIncomes, blackIncomeCounts, width = 0.9, color = "b", label = "Black")
plt.bar(apiIncomes, apiIncomeCounts, width = 0.9, color = "m", label = "Asian-Pac-Islander")
plt.bar(aieIncomes, aieIncomeCounts, width = 0.9, color = "y", label = "Amer-Indian-Eskimo")
plt.bar(otherIncomes, otherIncomeCounts, width = 0.9, color = "k", label = "Other")

plt.xlabel("Incomes")
plt.ylim([0, 3000])
plt.ylabel("Counts of incomes")
plt.legend()
plt.title("Income distribution by race (graph capped at 3000 - White has a lot more data)")
plt.show()        



nativeCountry = df2["native.country"]
nativeCountry.value_counts()                                                    # Ok, will graph native countries excluding the USA and Mexico (this would distort the graph as it is the majority of the data)

nativeCountries = nativeCountry.value_counts().index.tolist()                   # Will need to drop United-States and Mexico in a moment
nativeCountriesCount = nativeCountry.value_counts().tolist()

nativeCountries.pop(0)
nativeCountries

nativeCountriesCount.pop(0)
nativeCountriesCount

plt.figure(figsize = (20, 10))

colours = ["blue", "yellow"]

plt.bar(nativeCountries, nativeCountriesCount, width = 0.5, color = colours)

plt.xlabel("Countries", fontsize = 15)
plt.xticks(rotation = 90, fontsize = 15)
plt.ylabel("Income counts from country", fontsize = 15)
plt.title("Income distribution by country", fontsize = 15)

plt.show()        




print(df2.columns)  

# I want the distinct values for the columns with the categorical data

df3 = df2[["workclass",
           "education",
           "marital.status",
           "occupation",
           "relationship",
           "race",
           "sex",
           "native.country",
           "income"]]   

df3

for i in df3.columns:
    print(df3[i].value_counts())
    
'''
WORKCLASS (7):
Private             22286
Self-emp-not-inc     2499
Local-gov            2067
State-gov            1279
Self-emp-inc         1074
Federal-gov           943
Without-pay            14

EDUCATION (16):
HS-grad         9840
Some-college    6678
Bachelors       5044
Masters         1627
Assoc-voc       1307
11th            1048
Assoc-acdm      1008
10th             820
7th-8th          557
Prof-school      542
9th              455
12th             377
Doctorate        375
5th-6th          288
1st-4th          151
Preschool         45

MARITAL STATUS (7):
Married-civ-spouse       14065
Never-married             9726
Divorced                  4214
Separated                  939
Widowed                    827
Married-spouse-absent      370
Married-AF-spouse           21

OCCUPATION (14):
Prof-specialty       4038
Craft-repair         4030
Exec-managerial      3992
Adm-clerical         3721
Sales                3584
Other-service        3212
Machine-op-inspct    1966
Transport-moving     1572
Handlers-cleaners    1350
Farming-fishing       989
Tech-support          912
Protective-serv       644
Priv-house-serv       143
Armed-Forces            9

RELATIONSHIP (6):
Husband           12463
Not-in-family      7726
Own-child          4466
Unmarried          3212
Wife               1406
Other-relative      889

RACE (5):
White                 25933
Black                  2817
Asian-Pac-Islander      895
Amer-Indian-Eskimo      286
Other                   231

SEX (2):
Male      20380
Female     9782

NATIVE COUNTRY (41):
United-States                 27504
Mexico                          610
Philippines                     188
Germany                         128
Puerto-Rico                     109
Canada                          107
India                           100
El-Salvador                     100
Cuba                             92
England                          86
Jamaica                          80
South                            71
Italy                            68
China                            68
Dominican-Republic               67
Vietnam                          64
Guatemala                        63
Japan                            59
Columbia                         56
Poland                           56
Taiwan                           42
Iran                             42
Haiti                            42
Portugal                         34
Nicaragua                        33
Peru                             30
Greece                           29
France                           27
Ecuador                          27
Ireland                          24
Hong                             19
Cambodia                         18
Trinadad&Tobago                  18
Thailand                         17
Laos                             17
Yugoslavia                       16
Outlying-US(Guam-USVI-etc)       14
Hungary                          13
Honduras                         12
Scotland                         11
Holand-Netherlands                1

INCOME (2):
<=50K    22654
>50K      7508
'''    

# It would be good to have a visualisation of these categories

print(df3.columns)

categories = ['workclass (7)', 'education (16)', 'marital.status (7)', 'occupation (14)',
              'relationship (6)', 'race (5)', 'sex (2)', 'native.country (41)', 'income (2)']

counts = [7, 16, 7, 14, 6, 5, 2, 41, 2]

plt.pie(counts,
        labels = categories,
        explode = (0, 0, 0, 0, 0, 0, 0, 0, 0.5),
        shadow = True,
        startangle = 180,
        autopct = "%1.1f%%")

plt.title("Counts of categories")
plt.show()                                                                                           

'''
The target variable (income) is not even - this will distort machine learning models:
INCOME (2):
<=50K    22654
>50K      7508
Sample for 7000 of each income type
'''

under50k = df2[df2["income"] == "<=50K"]
over50k = df2[df2["income"] == ">50K"]

print(under50k.describe())                                    # 22654
print(over50k.describe())                                     # 7508

under50k_sample = under50k.sample(n = 7000)                   # SAMPLES CHANGE IF YOU RERUN THE CODE
over50k_sample = over50k.sample(n = 7000)

df4 = concat([under50k_sample, over50k_sample])
df4 = df4.reset_index(drop = True)
df4    

df4["income"].value_counts()                                  # Now evenly sampled, thus fair to apply ml to



# Make a df with categorical variables only - then apply a ml model
# Make a df with numerical variables - then apply a ml model (will drop capital gain/loss as I feel the data is insufficient)

df5 = df4[["workclass", "education", "marital.status", 
           "occupation", "relationship", "race", "sex",
           "native.country", "income"]]

df6 = df4[["fnlwgt", "education.num", "hours.per.week", "income"]]

df5

df7 = df5.apply(LabelEncoder().fit_transform)                 
df7

X = df7.drop("income", axis = 1)      
y = df7["income"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)

DTC = DecisionTreeClassifier()       
DTC.fit(X_train, y_train)
accuracy_score(y_test, DTC.predict(X_test))                                # For this sample 75.1% accuracy 

cm = confusion_matrix(y_test, DTC.predict(X_test))                         # Predictions fairly even - more high incomes predicted                                      
cm

DTC.predict([[1,1,1,1,1,1,1,1]])                                           # Model is able to make predictions

BC = BaggingClassifier(DTC)
BC.fit(X_train, y_train)
print(accuracy_score(y_test, BC.predict(X_test)))                          # Bagging improves accuracy to 75.6%

ABC = AdaBoostClassifier(base_estimator = DTC, random_state = 42)          
ABC.fit(X_train, y_train)
accuracy_score(y_test, ABC.predict(X_test))  

GBC = GradientBoostingClassifier()                             
GBC.fit(X_train, y_train)
GBC.score(X_test, y_test)                                                   # Gradient boosting --> 78.3%

parameters = {"learning_rate": [0.01, 0.1, 1], 
              "n_estimators": [50, 100, 150, 200, 250]}

GBC_model = GridSearchCV(GBC, parameters, cv = 3)                           # I wanted to do this with more parameters, but it was taking too long
GBC_model.fit(X_test, y_test)   

cvresults = DataFrame(GBC_model.cv_results_)
cvresults

GBC2 = GradientBoostingClassifier(n_estimators = 200)                       # Optimal for n_estimators = 200 and the default learning_rate of 0.1
GBC.fit(X_train, y_train)
GBC.score(X_test, y_test)                                                   # Accuracy peaking at 78.3%

df6

inc = get_dummies(data = df6.income, drop_first = True)
inc

df8 = concat([df6, inc], axis = 1)                                          # INCLUDE AXIS = 1 OTHERWISE IT CONCATS STRANGELY
df8

df8.drop("income", axis = 1, inplace = True)
df8

df8 = df8.rename(columns = {'>50K': 'income'})
df8                                                                         # DataFrame now ready for ml models

X = df8.drop("income", axis = 1)          
y = df8.income                            

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

LogR = LogisticRegression()

LogR.fit(X_train, y_train)

LogR.score(X_test, y_test)                                                  # Accuracy with logistic regression is 50.5% - still very low compared to the classification ml models

LogR.predict([[2000, 18, 75]])

cm = confusion_matrix(y_test, LogR.predict(X_test))                                                                        
cm                                                                          # The model never predicts an income >50k, so it's worthless - use classification models from earlier.
